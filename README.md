# datalessdb

## 1. Description
A custom sqlite engine that you can stream data over a socket connection from a data generator instead of reading from disk.

## 2. Files
After compiling files, there are 4 main executable output:
* The original sqlite3
* The modified sqlite3
* The modified sqlite without prints
* Data Generator

## 3. Dependencies
There is no strict dependency for the project. GNU make command is enough to compile. The program is able to run only in Unix/MacOS platforms.

## 4. Compilation
In the folder

```
make
```

command should be executed. 

## 5. Execution
### 5.1. The original sqlite3
This version of sqlite takes one argument which is the data for the initialization. We mostly use TPC-H data for the tests. It can be generated by using tools, or can be downloaded. The following should be enough to generate data 
[electrum/tpch-dbgen](https://github.com/electrum/tpch-dbgen)

After having the data ready (let's call it tpch.db). The following command should be executed to run sqlite.

```
./sqlite3 tpch.db
```

### 5.2. The modified sqlite3
Executing the script is enough. The only important thing is to run the script in the root folder to let the script set path parameters properly.
 
 ```
./sqlite3_stream
```

After getting into sqlite3 console, following example query can be executed. It might take a while to finish since 'order' table has 1.5 Million records.

 ```
select * from 'order';
```

Here since there is a collision between the table name and order command in sqlite, it is required to use single quotes around the table name.

### 5.3. The modified sqlite3 without prints
Same steps in 5.2 also apply here. Only change is the file name ´./sqlite3_stream_np´.

### 5.4. Data Generator
This script is executed by modified sqlite3 scripts automatically. It basically generates and streams data for the given tables. There is no need to execute it manually.

## 6.Configuration and Notes
* I tried to make things as autonomous as possible. Yet, the table structure plays an important role. So, I would suggest to start with order table since I configured generator for this specific table. Table structures are loaded from the file 'db_schema.txt'. So, if another kind of table structure is desired, then this file should be edited.

* In the 'sqlite_stream.h' file, the server part of the streamer is located meaning that it gets the data from the generator. I strongly recommend reading the source code in this file to understand the data reading process. TPC-H table structures are hardcoded here since it wasn't possible to read them in sqlite3 engine. And every table has a unique id which is named as 'table_id'. Normally, this parameter should be passed into the generator but currently generator assumes that the table is 'order'. So, if this project is desired to expand, generator should take table_ids dynamically.

* Also every table has the column structure in 'sqlite_stream.h' file to set these variables in the db engone properly. These are also hardcoded. An improvement would be reading the 'db_schema.txt' and setting these parameters automatically. Column types have a unique identifier:
  string  : 0
  integer : 1
  date    : 2
 
 * Data generator should be improved to support different tables and generation types. Currently it is not even generating random data but constant data which means that streaming the same data all the time. Random integer, String and Date data generation should be implemented.
 
 * The 'sqlite3_modified.c' is a huge file this is why I tried to seperate streaming logic from the engine as much as possible. Yet, to integrate them, I had to change it a bit. At first, a function called 'initializeDatabase' is called to create table schema. After that the initialization parameters are set: 'erso_init' and 'initialized'. Most of the time the initialization occurs with the first query in the sqlite console.
 
 * One can find the changed parts of the engine by searching 'erso_init', since I check if the db initialized before every streaming process. Currently only projections and selections are supported, aggregated queries and join operations need further work. If someone wants to improve streaming process for other operations. 'Explain queries' are good way to find out which parts of the code should be changed.
  
## 7.Testing

* I also added 'sqlite_stream.c' file, which is only for testing 'sqlite_stream.h'. It might be changed as required to test and understand the streaming server.

* To test different queries, the bash command below might be helppful. It basically only gets the last line of the query result which is the execution time information and saves it into RESULT.txt. This might be put into a bash script file to automatize tests. If noprint version of sqlite3 is used then ´tail -n 1´ part can be neglected.

 ```
echo "select * from 'order' ;" | ./sqlite3_stream | tail -n 1  >> RESULTS.txt;
```
